from datetime import datetime
import re
from collections import deque
import json
''' Builds Rinex 3.05 file 

    This file is quite messy. The data
    structure at the top that is used for some hardcoded header values for the 
    RINEX 3.05 file. 
'''


H_d = { # If there is time, move this to a json file for easier editing.
    'ver':'3.05',
    'file_type':'Observation Data', # We only Generate Observation files
    'satellite_systems':'G', 
    'PGM':'ConRINEX3', # name to output for 
    'date':'Date Generation Error', # Date should be generated by header_date method
    'comment1':'Teqc Compiler information -NEEDED?-',
    'comment2':'BIT 2 OF LLI FLAGS DATA COLLECTED UNDER A/S CONDITION',
    'marker_name':'PLACEHOLDER',
    'marker_type':'GEODETIC', 
    'observer': 'SURREAL LAB',         
    'agency': 'UVIC', 
    'recv_num':'placeholder',
    'recv_type':'placeholder',
    'recv_sftware': 'placeholder', # Version number for firmware on receiver
    'ant_num':'placeholder',
    'ant_type':'placeholder',
    'approx_XYZ':'get from rinex2.11 file',
    'ant_delta_HEN':'get from rinex2.11 file',
    'sys_num_obs_type':'get from epoch data',
    'interval': 'get from meta data',
    'time_of_first':'get from old header',
    'sys_phase_shift': 'Hard coded in JSON file',
    'glonass_slot_freq': 'Hard coded in JSON file',
    'glonass_cod_phs_bis': 'Hard coded in JSON file',
    'leap_seconds': 'from meta data file',
}

def header_date():
    ''' Creates the string representing the time the output file was created '''
    now = datetime.utcnow().timetuple()
    date_str = f'{now.tm_year}{now.tm_mon:02}{now.tm_mday:02} {now.tm_hour:02}{now.tm_min:02}{now.tm_sec} UTC'
    return date_str

def sys_obs_types(Log, tracked_signals: list) -> list:
    signal_lines = []
    for obs in tracked_signals:
        line = obs.strip('\n').split(' ')
        sys = line[0]
        signals = deque(line[1:])
        sys_obs_line = f'{sys}{" ":2}{len(signals):3}'
        count = 0
        while len(signals) > 0 :
            count += 1
            sys_obs_line += f' {signals.popleft():3}'
            if count == 13:
                sys_obs_line += f'  SYS / # / OBS TYPES'
                signal_lines.append(sys_obs_line)
                sys_obs_line = f'{" ":6}'
                count = 0
        if count != 0:
            signal_lines.append(f'{sys_obs_line:60}SYS / # / OBS TYPES')
            


    return signal_lines

def filename_time_interval(Log, interval):
    ''' Creates the time interval for the output file name 
    
        Done per the RINEX 3.05 standard. If nothing matches,
        uses the 00U identifier as that is for unknown.
    '''
    interval = float(interval)
    if interval < .01:
        return f'{.01/interval:2}C'
    if interval < 1:
        return f'{1/interval:2}Z'
    interval = int(interval)
    if interval == 1:
        return f'0{interval:1}S'
    if interval < 60:
        return f'{interval:2}S'
    interval = int(interval/60)
    if interval < 60:
        return f'{interval:2}M'
    interval = int(interval/60)
    if interval < 24:
        return f'{interval:2}H'
    interval = int(interval/24)
    if interval < 100:
        return f'{interval}D'
    return "00U"
    
def sv_list(epoch_data):
    svs = list() 
    all_sv = epoch_data['all sv']
    for sv in all_sv:
        if sv[0] not in svs:
            svs.append(sv[0])
    return svs
def header(Log,svs, head_data: dict, epoch_data, json_dir) -> list:
    ''' Creates a list of strings representing the header of a rinex 3.05 
        observation file returns a list of the lines to be written to the header

    '''
    teqc_head_d = dict()
    tracked_signals = epoch_data['tracked signals']
    all_sv = epoch_data['all sv']
        
    for line in tracked_signals:
        if line[0] not in svs:
            tracked_signals.remove(line)
    if tracked_signals[-1][0] not in svs: 
        tracked_signals.remove(tracked_signals[-1])
    sv_sig_list = sorted(epoch_data['sig by sys'])
    for item in head_data['meta']:
        line = item.strip('\n').split(':')
        key = line[0]
        data = ':'.join(line[1:])
        data = re.sub('^ +','', data)
        teqc_head_d[key] = data
        Log('D',f'meta data >> {key}:{data}')
    ## Formatting the information from the meta data file
    ## APPROX POSITION XYZ
    ## ANTENNA: DELTA H/E/N
    antenna_delta = teqc_head_d['antenna height (m)']
    antenna_delta_zero = '0.0000'
    teqc_head_d["ANTENNA: DELTA H/E/N"] = f'{antenna_delta:>14}{antenna_delta_zero:>14}{antenna_delta_zero:>14}'
    ## RECEIVER INFO
    H_d["recv_num"]     = teqc_head_d['receiver ID number']
    H_d["recv_type"]    = teqc_head_d['receiver type']
    H_d["recv_sftware"] = teqc_head_d['receiver firmware']
    ## ANTENNA INFO
    H_d["ant_num"] = teqc_head_d['antenna ID number']
    H_d["ant_type"] = teqc_head_d['antenna type']
    for item in head_data:
        if item == 'meta':
            continue
        a = head_data[item]
        for x in a:
            if x not in teqc_head_d.keys():
                teqc_head_d[x] = a[x]
    HL = list() # list to hold the header data lines
    # NOTE : for fstrings, to limit line length you might see [:xx]:xx} which prevents the string 
    #        from being longer or shorter than xx
    HL.append(f'{H_d["ver"]:>9}{" ":11}{H_d["file_type"]:1}{" ":4}{H_d["satellite_systems"]}{" ":19}RINEX VERSION / TYPE')
    HL.append(f'{H_d["PGM"]:20}{H_d["agency"]:20}{header_date():20}PGM / RUN BY / DATE')
    HL.append(f'{"COMMENTS PREFACED WITH (T) ARE GENERATED BY TEQC":60}COMMENT')
    HL.append(f'{"MULTIPLE V2.11 (EXTENDED) OBSERVATION FILES CREATED BY":60}COMMENT')
    HL.append(f'{"TEQC PROGRAM ARE COMBINED TO CREATE THE V3 FILE":60}COMMENT')
    HL.append(f'(T){teqc_head_d["COMMENT"][1][:57]:57}COMMENT')
    HL.append(f'{"OBSERVATION DATA, LLI AND SSI INDICATORS ARE UNCHANGED":60}COMMENT')
    HL.append(f'{"LLI MEANING DIFFERS BETWEEN VERSIONS 2 AND 3 AND THE":60}COMMENT')
    HL.append(f'{"INTERPRETATION OF BIT 1 AND 2 SHOULD BE USED WITH CAUTION":60}COMMENT')
    HL.append(f'{teqc_head_d["station name"].upper():60}MARKER NAME')
    HL.append(f'{teqc_head_d["station ID number"]:60}MARKER NUMBER')
    HL.append(f'{H_d["marker_type"]:20}{" ":40}MARKER TYPE')
    HL.append(f'{H_d["observer"]:20}{H_d["agency"]:40}OBSERVER / AGENCY')
    HL.append(f'{H_d["recv_num"]:20}{H_d["recv_type"]:20}{H_d["recv_sftware"]:20}REC # / TYPE / VERS')
    HL.append(f'{H_d["ant_num"]:20}{H_d["ant_type"]:20}{" ":20}ANT # / TYPE')
    HL.append(f'{teqc_head_d["APPROX POSITION XYZ"]:60}APPROX POSITION XYZ')
    HL.append(f'{teqc_head_d["ANTENNA: DELTA H/E/N"]:60}ANTENNA: DELTA H/E/N')
    HL.extend(sys_obs_types(Log,tracked_signals))

    HL.append(f'(T){teqc_head_d["COMMENT"][2][:57]:57}COMMENT')
    HL.append(f'(T){teqc_head_d["COMMENT"][3][:57]:57}COMMENT')
    HL.append(f'{"ASSUMED TEQC APPLIES PHASE SHIFT":<60}COMMENT')
    HL.extend(sys_phase_shift(Log, sv_sig_list, f'{json_dir}/sys_phase_shift.json') )
    interval = f'{float(teqc_head_d["sample interval"]):>10.3f}'
    HL.append(f'{interval:60}INTERVAL')
    HL.append(f'{teqc_head_d["TIME OF FIRST OBS"]:60}TIME OF FIRST OBS')
    d = datetime.strptime(teqc_head_d["final date & time"],"%Y-%m-%d %H:%M:%S.%f")
    end_time = f'{d.year:>6}{d.month:>6}{d.day:>6}{d.minute:>6}{d.second:>6}{d.microsecond:>13.7f}{"GPS":>8}'
    HL.append(f'{end_time:60}TIME OF LAST OBS')

    HL.append(f'{" ":60}END OF HEADER')
    return HL

def make_filename(Log,svs, meta,output_name,working_dir,station):
    ''' Generates the filename for the RINEX 3.05 OBS file

        The filename is either specified using the -out flag at runtime
        or is generated according the the RINEX 3.05 file naming standards.
    '''

    working_dir = working_dir.rstrip('/')
    if output_name is not None:
        return f'./{output_name}'
    teqc_head_d = dict()
    for item in meta:
        line = item.strip('\n').split(':')
        key = line[0]
        data = ':'.join(line[1:])
        data = re.sub('^ +','', data)
        teqc_head_d[key] = data
    if station == None:
        generated_filename = f'{teqc_head_d["4-char station code"].upper()}00CAN_R_'
    else:
        generated_filename = f'{station.upper()}00CAN_R_'
    start_time = datetime.strptime(teqc_head_d["start date & time"],"%Y-%m-%d %H:%M:%S.%f")
    end_time = datetime.strptime(teqc_head_d["final date & time"],"%Y-%m-%d %H:%M:%S.%f")
    generated_filename += f'{start_time.strftime("%Y%j%H%M")}_'
    period = (end_time - start_time).total_seconds() + int(float(teqc_head_d["sample interval"]))
    file_period = {900:"15M_",3600:"01H_",86400:"01D_",31536000:"01Y_"}
    try:
        generated_filename += file_period[period]
    except KeyError:
        generated_filename += "00U_"
    generated_filename += f'{filename_time_interval(Log, float(teqc_head_d["sample interval"]))}_'
    if len(svs) > 1:
        generated_filename += f'MO.rnx'
    if len(svs) == 1:
        generated_filename += f'{svs[0]}O.rnx'
    generated_filename.strip(' ')
    print(f'{generated_filename}')
    return f'{generated_filename}'

def sys_phase_shift(Log, sv_sig_list, sys_phase_json):
    ''' Gets the hardcoded glonass phase shift data

        Any satellites not observed will be skipped.
    '''
    with open(sys_phase_json, 'r') as fp:
        sys_phase = json.load(fp)
    sys_phase_shift_lines = []
    for sv_sig in sv_sig_list:
        try:
            sys_phase_shift_lines.append(f'{sys_phase[sv_sig]:60}{"SYS / PHASE SHIFT":<20}')
        except KeyError:
            pass
    return sys_phase_shift_lines

def glonass_slot_frq(Log, sv_list, glonass_json):
    ''' Creates the GLONASS SLOT / FRQ # header lines 
    
    '''
    with open(glonass_json, 'r') as fp:
        freq_num = json.load(fp)
    glonass_sv_list = list()
    slot_freq_list = []
    for sv in sv_list:
        if sv[0].lower() == 'r':
            glonass_sv_list.append(sv)
    line = f'{len(glonass_sv_list):>3} '
    for idx,sv in enumerate(glonass_sv_list):
        line += f'{sv:3} {freq_num[sv]} '
        if (idx+1)%8 == 0:
            slot_freq_list.append(f'{line:<60}{"GLONASS SLOT / FRQ #":<20}')
            line = '    '
    slot_freq_list.append(f'{line:<60}{"GLONASS SLOT / FRQ #":<20}')

    return slot_freq_list
   

def get_matches(Log, json_fname):
    ''' Creates a key-value mapping for finding data
    
        This is an important part of ConRINEX3. It uses the
        vendor specific json file to create a python dictionary
        that is used to determine which file the data is in, and which
        RINEX 2.11 observation matches it.
    '''
    with open(json_fname, 'r') as file_json:
        matches = json.load(file_json)
       # print(matches)
    empty_systems = []
    for k1 in matches.keys():
        empty_data = []
        for k2 in matches[k1]:
            if matches[k1][k2] == '':
                empty_data.append(k2)
        for key in empty_data:
            matches[k1].pop(key)
        if len(matches[k1].keys()) == 0:
            empty_systems.append(k1)
    for system in empty_systems:
        matches.pop(system) # removes the systems with no data matches
    matches_new = dict()
    for key in matches.keys():
        matches_new[key] = dict()
        for sig_key in matches[key]:
            matches_new[key][sig_key] = matches[key][sig_key] 
    return matches_new

def write_file(Log, lines: list, name: str):
    ''' Writes the RINEX 3.05 file

        Pretty straight forward. The lines are generated, then 
        passed to this function for writing.
    '''

    with open(name, 'w') as opF:
        for line in lines:
            opF.write(f'{line}\n')
    Log('I', f'{name} created')


def create_305(Log, data, input_info, output_order, output_name, json_dir,station):
    ''' creates a Rinex 3.05 observation file from input data 
    
        This is the nervous system of constructing the RINEX 3.05 file.
        The general idea is that it goes through each epoch of data that was
        contained in the RINEX 2.11 files, and checks to see what satellites
        were observed. For each satellite we want output for, it will use a 
        matching dictionary made from vendor specific json files. This dictionary
        contains the signals that we are interested in as keys, and splices the
        value into the file needed, and the signal needed.
        Args:
            Log
                Logging function for uniform, layered output.
            data (info in confluence)
                data scraped from the RINEX 2.11 files structured as follows
                data > Header : data used to generate the RINEX 3.05 header
                data > Header > rinex 2.11 filename 
                data > Header > rinex 2.11 filename > header key:value (see readme)
                data > Data : Observationd data
                data > Data > 2.11 filename > epoch > sv > Obs data
                    Obs data is key value observable identifier : observable value
            input_info
                configuration settings
            output_order
                the ordering of the signals for output
            json_dir
                the directory containing the json files
    '''
    matches = get_matches(Log, input_info['json'])
    data_header = data['Header']
    data_body = data['Data']
    filenames = [key for key in data_body.keys()]
    epochs = list()
    sv_by_epoch = dict()
    file_match_key = dict()
    all_sv = set()
    for filename in filenames:
         
        for epoch in data_body[filename].keys():
            if epoch not in epochs:
                epochs.append(epoch)
            for sv in data_body[filename][epoch].keys():
                if epoch not in sv_by_epoch.keys():
                    sv_by_epoch[epoch] = list()
                sv_by_epoch[epoch].append(sv)
                all_sv.add(sv)
        file_match_key['.'.join(filename.split('.')[1:])] = filename
    all_sv = sorted(list(all_sv))
    sv_systems = dict()
    tracked_signals = []
    for sys in output_order['sv_systems']:
        if sys in matches.keys():
            signal_list = list()
            for signal in output_order[sys]:
                if signal in matches[sys].keys():
                    signal_list.append(signal)
            tracked_signals.append(f'{sys.upper()} {" ".join(signal_list)}\n')
    output_lines = []
    
    sig_by_sys = set()
    for epoch in epochs:
        for sys in output_order['sv_systems']:
            sv_systems[sys] = list()
        sv_this_epoch = sorted(list(set(sv_by_epoch[epoch])))
        sv_count = 0
        for sv in sv_this_epoch:
            try:
                sv_systems[sv[0].lower()].append(sv)
                sv_count += 1
            except:
                pass # Skips systems we don't care about
        if sv_count < 10: 
            output_lines.append(f'> 20{epoch} 0{sv_count}')
        else:
            output_lines.append(f'> 20{epoch} {sv_count}')
        # print(f'> 20{epoch} {len(sv_this_epoch)}')
        for sys in output_order['sv_systems']:
            for sv in sv_systems[sys]:
                opStr = sv
                obs_count = 0
                for obs in output_order[sys]:
                    if sys not in matches.keys():
                        continue
                    if obs not in matches[sys].keys():
                        # Log('D',f'{sys}:{obs}')
                        continue
                    key = matches[sys][obs]
                    
                    if key != '':
                        filename = file_match_key[key.split(',')[0]]
                        sig_by_sys.add(f'{sys[0].upper()} {obs}')
                        obs = key.split(',')[1]
                        opStr += f'{data_body[filename][epoch][sv][obs]:16}'
                        obs_count += 1 # use this to skip satelites with 0 observables
                    else:
                        opStr +=   ''
                if obs_count > 1:
                    opStr = opStr.rstrip()
                    output_lines.append(opStr)
                else:
                    print(sv)
    # print(','.join(list(sig_by_sys)))
    epoch_data = {
        'tracked signals' :tracked_signals,
        'sig by sys' : sig_by_sys,
        'all sv' : all_sv
        }
    svs = sv_list(epoch_data)
    output_header = header(Log,svs, data_header, epoch_data, json_dir)
    now = datetime.now().timetuple()
    date_str = f'{now.tm_year}{now.tm_mon:02}{now.tm_mday:02}-{now.tm_hour:02}{now.tm_min:02}{now.tm_sec}'
    output_header.extend(output_lines)
    
    name = make_filename(Log,svs, data_header['meta'],output_name,input_info['working_dir'],station)
    write_file(Log, output_header, name)
    return name

